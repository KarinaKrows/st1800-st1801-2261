{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSUoHE5d2X2T"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Edwin Montoya-Múnera - emontoya@eafit.edu.co\n",
        "# Universidad EAFIT \n",
        "# 2022-1\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWBFHbzS20OQ",
        "outputId": "6688528e-4d45-4279-9066-12dc0ed9f90f"
      },
      "outputs": [],
      "source": [
        "#configuración en google colab de spark y pyspark\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSuD9g4H2X2Y"
      },
      "outputs": [],
      "source": [
        "# cargar las librerias necesarias\n",
        "## 1. nltk para 'procesamiento natural del lenguaje'\n",
        "## 2. pandas para procesamiento de dataframes, muy usado en preparación de datos\n",
        "## 3. re - expresiones regulares\n",
        "## 4. numpy, codecs, etc - otraas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztaFc9Rk3xx3",
        "outputId": "79655aa2-c8a7-4211-d15e-6bf2405b34d5"
      },
      "outputs": [],
      "source": [
        "!pip install nltk\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGC8GjQF2X2c"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import codecs\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCczY-GZ4hLN",
        "outputId": "2d553b2a-bbfa-4d93-cc3f-d476506248d6"
      },
      "outputs": [],
      "source": [
        "!ls 'gdrive/MyDrive/st1800github/datasets/gutenberg-es/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3mMCS952X2g"
      },
      "outputs": [],
      "source": [
        "# directorios (path) de entrada y salida:\n",
        "# \n",
        "path_in=\"gdrive/MyDrive/st1800github/datasets/gutenberg-es/\"\n",
        "path_out=\"gdrive/MyDrive/st1800github/out/\"\n",
        "filenametxt='pg2000.txt'\n",
        "filenamecleantxt='pg2000_clean.txt'\n",
        "filenamecsv='pg2000.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2PK3gGK2X2j",
        "outputId": "b3f87301-8ba4-4bd5-d073-772588efede2"
      },
      "outputs": [],
      "source": [
        "# corpus de nltk para 'tokenizer' y 'stopwords'\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhOXGmIm2X2n",
        "outputId": "7cb129df-3a9d-46c6-f261-05d9524927ce"
      },
      "outputs": [],
      "source": [
        "# ejemplo de como nltk tokeniza:\n",
        "texto=\"texto libre que permite crear     hiso1iras epor--4 no se preocupe \\n hola mundo cruel\"\n",
        "tokens = nltk.word_tokenize(texto)\n",
        "print(len(tokens))\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MavBnwgX2X2q",
        "outputId": "c2693046-a80e-4951-b98b-3b7d7e67f4d8"
      },
      "outputs": [],
      "source": [
        "# note la estrategia de tokenizar con sentencias simples de python, \n",
        "# ¿ cual le parece mejor?\n",
        "# y note la diferencia entre .split() y .split(' ')\n",
        "texto=\"texto libre que permite crear     hiso1iras epor--4 no se preocupe \\n hola mundo cruel\"\n",
        "tokens = texto.split()\n",
        "print(len(tokens))\n",
        "print(tokens)\n",
        "tokens = texto.split(' ')\n",
        "print(len(tokens))\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpRVLCb_5o5G",
        "outputId": "2fa0b203-aa0c-4c4f-a7d4-e9d60af499d6"
      },
      "outputs": [],
      "source": [
        "!pip install stop-words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hl9dZj6j2X2u",
        "outputId": "a4ef1bb8-6d7a-4281-be33-0e214b2327f3"
      },
      "outputs": [],
      "source": [
        "# stopwords con 'stop-words'\n",
        "# otra libreria diferentes de nltk para diccionario de stopwords, cual será mejor?\n",
        "# $ pip install stop-words\n",
        "# $ git clone --recursive git://github.com/Alir3z4/python-stop-words.git\n",
        "from stop_words import get_stop_words\n",
        "stop_words = get_stop_words('spanish')\n",
        "stop_words = get_stop_words('es')\n",
        "print(len(stop_words))\n",
        "print(stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02KLDTKB2X2y",
        "outputId": "5a7c92f7-81c0-4ee2-998c-2f9db486e45b"
      },
      "outputs": [],
      "source": [
        "# stopwords en nltk\n",
        "from nltk.corpus import stopwords\n",
        " \n",
        "stop_words_nltk = set(stopwords.words('spanish'))\n",
        "print(len(stop_words_nltk))\n",
        "print(stop_words_nltk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDnBhRPR6M7C",
        "outputId": "40e3c473-84c8-4ba9-b496-75ff2dd9888c"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWlr5PbI2X21",
        "outputId": "09bfed3c-2d88-4ce5-f473-f0d4889d1e81"
      },
      "outputs": [],
      "source": [
        "# permite verificar en nltk si un token pertenece a diccionario de un idioma, en este caso a 'english'\n",
        "from nltk.corpus import words as voc_en\n",
        "x = len(voc_en.words())\n",
        "print('tamaño del diccionario en ingles del nltk: ',x)\n",
        "# verifica si una palabra pertenece al diccionario:\n",
        "w = \"house\"\n",
        "if (len(w) >1) and w.isalpha() and (w in voc_en.words()) and (w not in stop_words):\n",
        "    print(w,\" true\")\n",
        "else:\n",
        "    print(w,\" false\")\n",
        "    \n",
        "w = \"pepito\"\n",
        "if (len(w) >1) and w.isalpha() and (w in voc_en.words()) and (w not in stop_words):\n",
        "    print(w,\" true\")\n",
        "else:\n",
        "    print(w,\" false\")    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9LUtX-b2X24"
      },
      "outputs": [],
      "source": [
        "# leer un archivo de ejemplo en .txt\n",
        "# input_file = open(path_in+filenametxt, \"r\", encoding='iso-8859-1')\n",
        "input_file = open(path_in+filenametxt, \"r\")\n",
        "filedata = input_file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "id": "MEOF2J422X27",
        "outputId": "a4ae0357-0128-459b-8cd7-33bd2cd461b6"
      },
      "outputs": [],
      "source": [
        "# opción 1:\n",
        "# TOKENIZAR con .split(), \n",
        "# ELIMINAR tokens de long = 1\n",
        "# ELIMINAR caracteres que no sean alfanumericos y pasar todo a minuscula\n",
        "# REMOVER stop words con nltk\n",
        "# graficar los 20 términos más frecuentes:\n",
        "\n",
        "tokens = filedata.split()\n",
        "tokens = [re.sub(r'[^A-Za-z0-9]+','',w) for w in tokens]\n",
        "# tokens=[word for word in tokens if word.isalpha()] si en vez de re.sub(r'[^A-Za-z0-9]+','',w) hace esto, que pasa?\n",
        "tokens = [w.lower() for w in tokens if len(w)>1]\n",
        "tokens = [w for w in tokens if w not in stop_words_nltk]\n",
        "\n",
        "fdist = nltk.FreqDist(tokens)\n",
        "print('numero de palabras finales = ',len(fdist))\n",
        "topwords = fdist.most_common(20)\n",
        "print (topwords)\n",
        "x,y = zip(*topwords)\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.bar(x,y)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "id": "oZLa8qtp2X3C",
        "outputId": "9b1e0b6d-498b-46c5-bc46-2a4ac8bb58d3"
      },
      "outputs": [],
      "source": [
        "# opción 2:\n",
        "# TOKENIZAR con nltk, \n",
        "# ELIMINAR tokens de long = 1\n",
        "# ELIMINAR caracteres que no sean alfanumericos\n",
        "# REMOVER stop words\n",
        "# graficar los 20 términos más frecuentes:\n",
        "\n",
        "tokens = nltk.word_tokenize(filedata)\n",
        "tokens = [w.lower() for w in tokens if len(w)>1]\n",
        "#tokens = [re.sub(r'[^A-Za-z0-9]+','',w) for w in tokens]\n",
        "tokens = [w for w in tokens if w not in stop_words_nltk]\n",
        "\n",
        "fdist = nltk.FreqDist(tokens)\n",
        "topwords = fdist.most_common(20)\n",
        "print('numero de palabras finales = ',len(fdist))\n",
        "print (topwords)\n",
        "x,y = zip(*topwords)\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.bar(x,y)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "id": "ryzIOXom2X3F",
        "outputId": "a91baf9c-149c-4f46-8558-731bee118958"
      },
      "outputs": [],
      "source": [
        "# Stemming con NLTK\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "snowball = SnowballStemmer(\"spanish\")\n",
        "\n",
        "# probar cada una de las siguientes opciones: porter y lancaster.\n",
        "#tokens = [porter.stem(w) for w in tokens]\n",
        "#tokens = [lancaster.stem(w) for w in tokens]\n",
        "tokens = [snowball.stem(w) for w in tokens]\n",
        "\n",
        "\n",
        "fdist = nltk.FreqDist(tokens)\n",
        "topwords = fdist.most_common(20)\n",
        "print('numero de palabras finales = ',len(fdist))\n",
        "x,y = zip(*topwords)\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.bar(x,y)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PtVeftQ_78Y",
        "outputId": "357e1c33-6882-4ebe-9fa1-4c60d1d364c5"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "id": "peWt2YhW2X3I",
        "outputId": "75c1e811-c9bd-4fb5-a16b-c977291f026e"
      },
      "outputs": [],
      "source": [
        "# Lemmatization con NLTK\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# probar cada una de las siguientes opciones: \n",
        "#tokens = [wordnet_lemmatizer.lemmatize(w, pos=\"v\") for w in tokens ]\n",
        "tokens = [wordnet_lemmatizer.lemmatize(w) for w in tokens ]\n",
        "\n",
        "fdist = nltk.FreqDist(tokens)\n",
        "topwords = fdist.most_common(20)\n",
        "print('numero de palabras finales = ',len(fdist))\n",
        "x,y = zip(*topwords)\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.bar(x,y)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtPGovDV2X3L"
      },
      "outputs": [],
      "source": [
        "# volver a leer el archivo ejemplo en .txt\n",
        "#input_file = open(path_in+filenametxt, \"r\",encoding='iso-8859-1')\n",
        "input_file = open(path_in+filenametxt, \"r\")\n",
        "output_file_clean = open(path_out+filenamecleantxt, \"w\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QM7UcDv2X3O"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "snowball = SnowballStemmer(\"spanish\")\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "for line in input_file:\n",
        "    line_clean = \"\"\n",
        "    tokens = nltk.word_tokenize(line)\n",
        "    #tokens = [re.sub(r'[^A-Za-z0-9]+','',w) for w in tokens]\n",
        "    tokens = [w.lower() for w in tokens if len(w)>1]\n",
        "    tokens = [w for w in tokens if w.isalpha()]\n",
        "    tokens = [w for w in tokens if w not in stop_words_nltk]\n",
        "    #tokens = [wordnet_lemmatizer.lemmatize(w, pos=\"v\") for w in tokens]\n",
        "    tokens = [wordnet_lemmatizer.lemmatize(w) for w in tokens]\n",
        "\n",
        "    #tokens = [porter.stem(w) for w in tokens]\n",
        "    #tokens = [lancaster.stem(w) for w in tokens]\n",
        "    tokens = [snowball.stem(w) for w in tokens]\n",
        "    \n",
        "    for w in tokens:\n",
        "        line_clean=line_clean+w+\" \"\n",
        "            \n",
        "    if (line_clean!=\"\"):\n",
        "        line_clean=line_clean+\"\\n\"\n",
        "        output_file_clean.write(line_clean)\n",
        "output_file_clean.close()        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbjK5ojJ2X3S"
      },
      "outputs": [],
      "source": [
        "input_file_clean = open(path_out+filenamecleantxt, \"r\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        },
        "id": "Szskm7XG2X3V",
        "outputId": "06140b8f-a9b9-4b55-d736-7d05f76d85c7"
      },
      "outputs": [],
      "source": [
        "filedata = input_file_clean.read()\n",
        "tokens = filedata.split()\n",
        "fdist = nltk.FreqDist(tokens)\n",
        "topwords = fdist.most_common(20)\n",
        "print('numero de palabras finales = ',len(fdist))\n",
        "x,y = zip(*topwords)\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.bar(x,y)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJo0gmxN2X3Z"
      },
      "outputs": [],
      "source": [
        "word_freq = fdist.most_common(len(fdist))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTh3-gfO2X3b"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "with open(path_out+filenamecsv, 'w') as csvFile:\n",
        "    writer = csv.writer(csvFile)\n",
        "    writer.writerow([\"word\", \"frecuency\"])\n",
        "    writer.writerows(word_freq)\n",
        "\n",
        "csvFile.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3ByuO1e2X3e",
        "outputId": "776a4cc5-cdf4-4a50-b7ad-1c04621f6d0d"
      },
      "outputs": [],
      "source": [
        "# extract top 30 words\n",
        "top_words = word_freq[:20]\n",
        "print(top_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZRfEi-d2X3i"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(top_words)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmA9Dccs2X3k"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "x,y = zip(*top_words)\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.bar(x,y)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fi0zEq9-2X3o"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "df = pd.DataFrame(top_words)\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.bar(df[0],df[1])\n",
        "plt.xticks(rotation=45)\n",
        "plt.xlabel(\"Word\")\n",
        "plt.ylabel(\"frecuency\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVe4jqDB2X3r"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "lab1-nltk.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.3 64-bit ('base': conda)",
      "metadata": {
        "interpreter": {
          "hash": "cad8c4fdaa4b956157b2a34b41019c2c8a1a3e67ee4cfdc26a8585e73fdbbd4a"
        }
      },
      "name": "Python 3.8.3 64-bit ('base': conda)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3-final"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
